\documentclass{article}

\input{preamble.tex}
\usepackage{enumitem}

\begin{document}

\title{A one phase IPM for non-convex optimization}
\author{Oliver Hinder, Yinyu Ye}

\algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
\algtext*{Indent}
\algtext*{EndIndent}

\maketitle


\newcommand{\algorithmicbreak}{\textbf{break}}
\newcommand{\obj}{f}
\newcommand{\cons}{a}
\newcommand{\parNumCor}{c_{\max}}
\newcommand{\parComp}{\beta_{1}}
\newcommand{\parCompAgg}{\beta_{2}}
\newcommand{\parFracBoundary}{\beta_{3}}
\newcommand{\parMinStableStepSize}{\beta_{4}}
\newcommand{\parKKTReductFactor}{\beta_{5}}
\newcommand{\parObjReductFactor}{\beta_{6}}

%\newcommand{\Filter}{\mathbb{F}}
\newcommand{\History}{\mathbb{H}}



\newcommand{\TOLinf}{\epsilon_{\textbf{inf}}}
\newcommand{\TOLmu}{\epsilon_{\textbf{mu}}}
\newcommand{\TOL}{\epsilon}

\newcommand{\status}{\textbf{status}}
\newcommand{\success}{\textsc{success}}
\newcommand{\failure}{\textsc{failure}}

\newcommand{\feasible}{\textbf{feasible}}

\newcommand{\meritKKT}{\mathbb{K}}

% alg
\newcommand{\simpleIPM}{Simplified-One-Phase-Non-Convex-IPM}
\newcommand{\callSimpleIPM}{\Call{Simplified-One-Phase-Non-Convex-IPM}}

\abstract{
Based on the work of [REF] it has been assumed that infeasible start IPM developed for conic optimization [REF] cannot be adapted to non-linear optimization and converge without significant modification i.e. a two phase method or a penalty function. We show that, in fact, by careful initialization and non-linear updates on the primal variables.
}

\section{Introduction}

\cite{wachter2000failure} showed that if we apply an infeasible start IPM to:
\begin{flalign*}
\min { x }\\
x^2 - s_1 - 1 &= 0 \\
x - s_2 - 1/2 &= 0 \\
s_1, s_2 &\ge 0
\end{flalign*}

Fails to converge to either a local optimum or infeasibility certificate

Modifications:
\begin{enumerate}
\item Two phases (IPOPT)
%\begin{itemize}
%\item Each phase has a different variables
%\item Awkward convergence assumptions
%\item Poor performance on infeasible problems
%\end{itemize}
\item Compute two directions (KNITRO)
\item Penalty (or big-M) method e.g. \cite{Chen06,curtis2012penalty}
\end{enumerate}


\subsection{Relevant literature}

Within non-convex optimization there are four papers that I think are particularly relevant to our work:

\begin{enumerate}
\item The paper \cite{wachter2000failure} shows that there are examples for which infeasible start algorithms will always fail to converge to either a optimal solution or a stationary measure of infeasibility when constraints are non-convex (irrespective of the strategy for used). This is the inspiration for the two phase algorithm of IPOPT and justifies why our one phase algorithm is necessary.
\item The description of the IPOPT algorithm \cite{wachter2005line}. IPOPT uses a two phase method the primary phase searches simultaneously for optimality and feasibility using a classical infeasible start method and a feasibility restoration phase that minimizes infeasibility. The feasibility restoration phase is only called when the step size for the infeasible start method is small. Another distinct feature of the algorithm is the filter line search (which allows progress on either the constraints or the objective).
\item The description of the KNITRO algorithm \cite{byrd2006knitro}. KNITRO is a trust region algorithm. The approach is quite distinct from typical infeasible start algorithms and is worth looking at (each step computes two different directions, using two different linear systems, one to reduce the objective and the other to reduce infeasibility). There is a more recent paper \cite{nocedal2014interior} that adds an feasibility restoration phase (this is theoretically unnecessary, but the practical results are good). 
\item The paper \cite{curtis2012penalty} introduces a barrier penalty method. This paper uses a similar approach to us. The main different with our approach is we treat $\lambda$ as a dual variable, whereas in Curtis's paper $\lambda$ is replaced by a penalty parameter that is updated in an ad hoc fashion.
\item Papers in convex optimization?
\end{enumerate}

\section{Motivation and derivation of algorithm}

We wish to solve the following problem:
\begin{flalign*}
 &  \min{f(x) } \\
& a(x) \le 0 \\
\end{flalign*}

Assume:
\begin{enumerate}
\item The constraints and objective are $C^2$ - $L_{0}, L_{1}$ etc
\item The set $a(x) \le \theta$ is bounded for any $\theta \in \mathbb{R}^{m}$
\end{enumerate}

\subsection{Infeasibility certificates}

\emph{First-order  local L1-infeasibility certificate}, if $x^{*}$ is a first-order local optimum of:
\begin{flalign*}
\min e^T z \\
a(x) - s + z = 0 \\
 s, z \ge 0
\end{flalign*}
With $e^T z > 0$.


\emph{First-order  local farkas infeasibility certificate}, if there exists some $w \le 0$, such that $x^{*}$ is a first-order local optimum of:
\begin{flalign*}
\min w^T a(x)
\end{flalign*}
With $w^T a(x) > 0$. Now, suppose we have found a point such that
$$
\| w^T \nabla a(x) \|_{2} < \frac{w^T a(x)}{\| x - x^{*} \|_{2}}
$$
Now, if the function $w^T a(x)$ is convex then we have:
$$
w^T a(x^{*}) \ge w^T a(x) + w^T \nabla a(x) (x - x^{*}) > 0
$$
Therefore we declare a problem locally infeasible if our algorithm finds a point with:
$$
\frac{\| w^T \nabla a(x) \|_{2}}{w^T a(x)} \le \TOLinf
$$
Where $\TOLinf$ is the tolerance for primal feasibility.

\todo{ $L_{\infty}$ ... }

\subsection{Derivation of method}


Log barrier problem:
\begin{flalign*}
 &  \min{f(x) - \mu^k \sum_i{ \log{(s_i)} } } \\
& a(x) + s = 0 \\
& s \ge 0
\end{flalign*}

Shifted log barrier problem
\begin{flalign}\label{shifted-barrier-problem}
 &  \min{f(x) -(1 - \eta^k) \mu^k \sum_i{ \log{s_i} } } + \frac{\delta^k}{2} \| x - x^k \|^2 \\
& a(x) + s = (1 - \eta^k) (a(x^k) + s^k) \\
& s \ge 0
\end{flalign}

KKT system:

\begin{flalign*}
\nabla f(x) + \delta^k (x - x^k) + {y}^T \nabla a(x) &= 0 \\
a(x) + s &= (1 - \eta^k) (a(x^k) + s^k) \\
s_i {y_i} &= (1 - \eta^k) \mu^k \\
s, y &\ge 0
\end{flalign*} 

Corresponding newton system:

\begin{flalign}\label{primal-dual-newton-direction}
\begin{bmatrix}
 \nabla^2_{x} L(x^k,y^k) + \delta^k I  & \nabla a(x^k)^T & 0  \\
\nabla a(x^k) & 0 & I \\
0 & S^k & Y^k
\end{bmatrix} 
\begin{bmatrix}
d_x^k \\
d_y^k \\
d_s^k
\end{bmatrix} 
=
\begin{bmatrix}
- (\nabla f(x^k) + \nabla a(x^k)^T y^k) \\
- \eta^k  (a(x^k) + s^k) \\
(1 - \eta^k) \mu^k e - Y^k s^k 
\end{bmatrix} 
\end{flalign} 

By taking the primal schur complement one can see solving system \eqref{primal-dual-newton-direction} is equivalent to solving:
$$
(M (x^k,y^k, s^k) + \delta I)  d_{x}^k = - \nabla f(x^k) - \nabla a(x^k)^T ((1 - \eta^k) \mu^k (S^k)^{-1} e + \eta^k Y^k ( e + (S^k)^{-1} (a(x^k) + s^k))  )
$$
Where:
\begin{flalign}
M (x,y, s) = \nabla^2 \Lag (x, y) + \nabla a(x)^T Y S^{-1} \nabla a(x) 
\end{flalign}

\begin{flalign}\label{terminate-when}
D(x,y) \le \mu^{k}  \& \| Sy - \mu^{k} \|_{\infty} \le \mu^k / 4
\end{flalign}

\subsection{Behavior of this algorithm on convex quadratic programs}

[one stabilization step followed by one aggressive step]. [resemblance to predictor corrector]

\section{Practical algorithm}


$$
\sigma (y) = \frac{100}{\max\{ 100, \| y \|_{\infty} \}}
$$



Scaled KKT termination criterion: 
\begin{subequations}\label{terminate-kkt}
\begin{flalign}
\sigma (y) \| \nabla L_{\gamma}(x, y) \|_{\infty} &\le  \epsilon  \\
\sigma (y) \| S y \|_{\infty} &\le \epsilon  \\
\| a(x) + s \|_{\infty} &\le \epsilon 
\end{flalign}
\end{subequations}
Local primal infeasibility termination criterion:
\begin{subequations}\label{terminate-primal-infeasible}
\begin{flalign}
\frac{\| w^T \nabla a(x) \|_{2}}{w^T a(x)} \le \TOLinf \\
\frac{\| w^T \nabla a(x) \|_{\infty}}{\| w \|_{\infty}} \le \TOLinf
\end{flalign}
\end{subequations}

Local dual infeasibility termination criteron
\begin{subequations}\label{terminate-dual-infeasible}
\begin{flalign}
\frac{\| w^T \nabla a(x) \|_{2}}{w^T a(x)} \le \TOLinf \\
\frac{\| w^T \nabla a(x) \|_{\infty}}{\| w \|_{\infty}} \le \TOLinf
\end{flalign}
\end{subequations}

Aggressive step criterion:
\begin{subequations}\label{agg-criteron}
\begin{flalign}
\| \nabla L_{\gamma}(x, y) \|_{\infty} &\le \min\left\{ \frac{\| a(x) + s \|_{\infty}}{\sigma (y)  } , 1 + \| \nabla f_{\gamma}(x) \|_{\infty} \right\} \\
 \frac{S y}{\mu} &\in [ e \parCompAgg, e / \parCompAgg ] 
\end{flalign}
\end{subequations}


\begin{algorithm}[H]
\textbf{Input:} Some initial point $x_{0}, y_{0}, s_{0}, \mu$ 
\vspace{0.2 cm} \\
\emph{For each iteration $i = 1, ..., i_{\max}$ perform the following steps:}
\begin{enumerate}[label*=A.{\arabic*}]
\item \label{step-1}  \emph{Update hessian and jacobian. 
\item Check the termination criterion given in \eqref{terminate-kkt}, \eqref{terminate-primal-infeasible} and \eqref{terminate-dual-infeasible}.
\item Form primal schur complement at the point $x$, $y$}
\item \emph{Factorize the matrix using IPOPT strategy}
\item \label{take-steps}  \emph{Perform corrections steps for $k \in \{ 1, ..., \parNumCor \}$}
\begin{enumerate}[label*=.{\arabic*}]
\item \emph{Perform corrections}
\begin{enumerate}[label=-Case-{\Roman*}]
\item If equation~\eqref{agg-criteron} is satisfied then go to the aggressive search direction algorithm (Algorithm~\ref{alg:aggressive}).
\item Otherwise go to the stablization search direction algorithm (Algorithm~\ref{alg:stable}).
\end{enumerate}
\item \emph{Deal with failures}. If correction succeeds set $x, y, s \gets x^{+}, y^{+}, s^{+}$. If correction failed and $k = 1$ go to \eqref{increase-delta-for-failure}. Otherwise if the correction failed with $k > 1$ go to step~\eqref{step-1}.
\end{enumerate}
\item Go to \eqref{step-1}
\item \label{increase-delta-for-failure} \emph{Increase delta to address failure}. Set $\delta = \max\{10 \delta, \delta_{\min} \}$ and factorize the matrix. Go to step \eqref{take-steps}.
\end{enumerate}
\caption{High level description of one phase IPM}
\end{algorithm}

\subsection{Updating the iterates}

Given a primal step size $\alpha_{P}$ we update the primal iterates as follows:
\begin{subequations}
\begin{flalign}\label{eq:iterate-update}
x^{+} &\gets x + \alpha_{P} d_{x} \\
\mu^{+} &\gets (1 - \eta \alpha_{P}) \mu \\
\theta^{+} &\gets (1 - \eta \alpha_{P}) \theta \\
s^{+} &\gets a(x^{+}) + \theta (  s^{1} - a(x^{1}) )
\end{flalign}
\end{subequations}
Fraction to the boundary:
\begin{flalign}
s + \alpha_{P} d_{s} \ge s \parFracBoundary
\end{flalign}
Given some candidate primal iterate $x^{+}$, $s^{+}$, we define $B( s^{+}, d_{y} )$ set of feasible dual step sizes. In particular, let it be the largest interval such that if $\alpha_{D} \in B( s^{+}, d_{y} )$ then
\begin{flalign}
 \frac{S^{+} y + \alpha_{D} d_{y}}{\mu} \in [\parComp, 1/\parComp ]
\end{flalign}
If no such interval exists we set $B( s^{+}, d_{y} )$ to the empty set and the step will be rejected. We compute the dual step size as follows:
\begin{flalign}
\alpha_{D} = \arg \min_{\alpha_{D} \in B( s^{+}, d_{y} )} \| S^{+} y + \alpha_{D} S^{+} d_{y} \|^2_{2} +  \sigma(y) \| \nabla L(x, y)  + (\nabla^2 L(x, y) + \delta I) d_{x} + \alpha_{D}  \nabla a(x)^T d_{y} \|^{2}_{2}
\end{flalign}
This reduces to a one dimensional least squares problem in $\alpha_{D}$ which has a closed form expression.

\subsection{Aggressive search directions}

\begin{algorithm}[H]
\textbf{Input:} Some point $x, y, s, \mu$ \\
\textbf{Output:} A new point $x^{+}, y^{+}, s^{+}, \mu^{+}$ and a $\status$
\begin{enumerate}[label*=A.{\arabic*}]
\item Compute a direction $d$ from the system~\eqref{primal-dual-newton-direction} with $\eta = 1$
\item Estimate the largest primal step size $\alpha^{\max}_{P}$
\item \emph{Perform a backtracking line search on the primal step $\alpha_{P}$.} Trial step sizes $\alpha \in \{\alpha^{\max}_{P}, \beta \alpha^{\max}_{P}, ... \}$ and terminate with $\status = \success$ the first time all of the following conditions hold:
\begin{enumerate}[label=({\roman*})] 
\item The fraction to the boundary rule \eqref{} is satisfied 
\item The set of valid dual step sizes is non-empty i.e. $B( s^{+}, d_{y} ) \neq \emptyset$ 
\item Lagragian???
\end{enumerate}
Terminate with $\status = \failure$ if the line search trials a point with
 $$
\alpha_{P} \le \textbf{some constant} \times \min_{\{ j : a_j(x) + s_j < 0 \}}{ \frac{ -s_j }{ a_j(x) + s_j} }
$$
\end{enumerate}
\caption{High level description of aggressive correction}\label{alg:aggressive}
\end{algorithm}

\subsection{Stablization search directions}


Let:
\begin{flalign}
\meritKKT ( x, y, s )  = \sigma( y ) \max\{ \| \nabla L(x, y ) \|_{\infty},  \| S y - \mu \|_{\infty} \}
\end{flalign}
During the stabilization search we accept iterates that sufficiently reduce the merit function:
\begin{flalign}
\phi(x^{+}, y^{+}, s^{+}) \le \phi(x, y, s) + \parObjReductFactor \tilde{\Delta}_{(x,y,s)}^{\phi}  ( d_{x}, d_{y} )
\end{flalign}
Or sufficient progress is made on a KKT merit function relative to the historic iterates. In particular, for every $(\hat{x}, \hat{y}, \hat{s}) \in \History$ one of the following two equations holds:
\begin{subequations}\label{eq:filter}
\begin{flalign}
\meritKKT (x^{+}, y^{+}, s^{+}) &\le (1 - \parKKTReductFactor \alpha_{P} ) \meritKKT (\hat{x}, \hat{y}, \hat{s}) \\
\phi(x^{+}, y^{+}, s^{+}) &\le \phi(\hat{x}, \hat{y}, \hat{s}) - \parKKTReductFactor \alpha_{P} \meritKKT (\hat{x}, \hat{y}, \hat{s}).
\end{flalign}
\end{subequations}
This filter accepts large steps that significantly reduce the KKT error. The motivation for this choice is that, even on non-convex problems, the optimization trajectory often passes through regions where the primal schur complement is positive definite. For example, this naturally occurs near points satisfying the sufficient conditions for local optimality. In these situation often the KKT error is a better measure of progress towards a local optimum than a merit function that discards information about the dual feasibility.

\begin{algorithm}[H]
\textbf{Input:} Some point $x, y, s, \mu$
\begin{enumerate}[label*=A.{\arabic*}]
\item Compute a direction $d$ from the system~\eqref{} with $\eta = 0$
\item Estimate the largest primal step size $\alpha^{\max}_{P}$
\item \emph{Perform a backtracking line search on the primal step $\alpha_{P}$.} Start with $\alpha_{P} = \alpha^{\max}_{P}$. Set $\status = \success$ and move to step~\eqref{filter-update} if all of the following conditions hold:
\begin{enumerate}[label=({\roman*})] 
\item The fraction to the boundary rule \eqref{} is satisfied 
\item The set of valid dual step sizes is non-empty i.e. $B( s^{+}, d_{y} ) \neq \emptyset$ 
\item The filter i.e. one of equations~\eqref{eq:filter} is satisfied.
\end{enumerate}
Return the original iterates and $\status = \failure$ if the line search trials a point with $\alpha_{P} \le \parMinStableStepSize$
\item \label{filter-update} Augment the histories $\History$ with the new iterate $(x^{+}, y^{+}, s^{+})$.
\end{enumerate}
\caption{High level description of stabilization corrections}\label{alg:stable}
\end{algorithm}

\begin{table}[H]
\begin{tabular}{ |c| p{7cm}|c|c| } 
 \hline
Parameter & Description & Possible values & Chosen value  \\ 
 \hline
$\parNumCor$ & Maximum number of corrections per iteration. See \eqref{}.  & Any natural number & $3$  \\ 
 \hline
  $\parComp$ & Restricts how far complementarity of $s$ and $y$ can be from $\mu$. See \eqref{}.  & The interval $(0,1)$ & 0.01 \\ 
 \hline
   $\parCompAgg$ & Restricts how far complementarity of $s$ and $y$ can be from $\mu$ in order for the aggressive criterion to be met. See \eqref{}.  & The interval  $(0, \parComp)$ &  \\ 
    \hline
   $\parMinStableStepSize$ & Minimum step size for stable line searches. See \eqref{}.  & The interval $(0,1)$ &  \\ 
   \hline
      $\parKKTReductFactor$ & Acceptable reduction factor for the scaled KKT error $\meritKKT$ during stabilization steps. See \eqref{}.  & The interval $(0,1)$ &  \\ 
      \hline
            $\parObjReductFactor$ & Acceptable reduction factor for the merit function $\phi$ during stabilization steps. See \eqref{}.  & The interval $(0,1)$ &  \\
\hline
\end{tabular}
\caption{Parameters values and descriptions}
\end{table}

\section{Convergence proofs}

\begin{lemma}
Assume that $a(x^{1}) - s^{1} = -\mu e$ and $\epsilon < 1$.
If the criterion for an aggressive step is met at iteration $k$ then we have:
$$
\| y^k \|_{1} \ge \frac{\| \nabla c(x^{k}) \|_{2}}{\TOL^2} + 3 m
$$
%$$
%s^{k} \ge \frac{\mu \TOL^2}{\| \nabla c(x^{k}) \|_{2} + 1}
%$$
\end{lemma}

\begin{proof}
Observe that:
$$-a(x)^T y = -(a(x) - s)^T y - s^T y \ge  \mu (e^T y - 2)$$
Therefore:
$$
\frac{\| \nabla a(x)^T y \|}{-a(x)^T y} \le \frac{\mu^k \sqrt{ \| y \|_{1} + 1} + \| \nabla c(x) \|}{\mu ( \| y \|_{1} - 2 m )} 
$$
If:
$$
\| y^k \|_{1} \ge \frac{\| \nabla c(x^{k}) \|_{2} +  3 m}{\TOL^2} 
$$
Then:
$$
\frac{\| \nabla a(x)^T y \|}{-a(x)^T y} \le \epsilon 
$$
Which gives the result.
\end{proof}

\begin{proof}
Observe that:
$$-a(x)^T y = -(a(x) - s)^T y - s^T y \ge  \mu (e^T y - 2)$$
Therefore:
$$
\frac{\| \nabla a(x)^T y \|}{-a(x)^T y} \le \frac{1+ \| \nabla c(x) \|}{\mu ( \| y \|_{1} - 2 m )} 
$$
If:
$$
\| y^k \|_{1} \ge \frac{\| \nabla c(x^{k}) \|_{2}}{\TOL^2} +  3 m
$$
Then:
$$
\frac{\| \nabla a(x)^T y \|}{-a(x)^T y} \le \epsilon 
$$
Which gives the result.
\end{proof}


\begin{lemma}
Assume that $a(x^{1}) - s^{1} = -\mu e$ and $\epsilon < 1$.
The algorithm, \callSimpleIPM{}, with any takes at most $\frac{\mu^{0} (2 \| \nabla c(x^{k}) \|_{2} + 8)}{\epsilon^2}$ aggressive steps to terminate with either a first-order stationary point or first-order farkas certificate.
\end{lemma}

\begin{proof}
We wish to prove that for any $\delta$ with
$$
\delta \ge \frac{\| g^k \| L_0}{\mu^k} - \lambda_{\min}{(M^{k})}
$$
and $\alpha$ satisfying
\begin{flalign}\label{assume-alpha}
\alpha \le \frac{1}{\| y^k \|_{\infty} + 4} %\frac{\min_{i}\{s^{k}_{i}\}}{4 \mu^k}
\end{flalign}
the iterate $x^{+} = x^{k} + \alpha d_{x}^k$, $y^{+} = y^{k} + \alpha d_{y}^k$, $\mu^{+} = \mu (1 - \alpha )$ is feasible. Observe that this implies the result since if:
$\alpha \ge \frac{1}{2 (\| y^k \|_{\infty} + 4)}$ %\frac{\min_{i}\{s^{k}_{i}\}}{4 \mu^k}
then:
 $$
 \mu^{k+1} = (1 - \alpha) \mu^{k} = \mu^k - \frac{\mu^k}{2 \| y^k \|_{\infty} + 8} \le \mu^k - \frac{\epsilon^2}{2 \| \nabla c(x^{k}) \|_{2} + 8}.
  $$
We wish to show that $ s^{k+1}  \in [s^k / 2, 3 s^{k} / 2]$. Where $s^{k+1} = a(x + \alpha_{P} d_{x}) + (1 - \alpha_{P} ) \mu^k e$. Subtracting and adding $s^k = a(x^k) + \mu^k e$ yields
$$
s^{k+1} = s^k + ( a(x^k + \alpha_{P}^k d_{x}^k ) - a(x^k)) - \alpha_{P}^k \mu^k e
$$
Therefore, it remains to bound the term $a(x^{k} + \alpha_{P}^k d_{x}^{k}) - a(x^k)  - \alpha_{P}^k \mu^k e$. Applying our assumption on $\alpha^{k}$, we immediately get $0 \le \alpha_P^{k} \mu^k e \le  s^{k} / 4$. Furthermore, we know that 
$\| d_{x}^{k} \|_{2} \le \mu^k L_0$ therefore:
$$
\alpha_{P}^{k} \| d_{x}^{k} \|_{2} \le \frac{\min_{i}\{ s^{k}_{i} \}}{2 L_{0}}
$$
Since $a(x)$ is $L_{0}$-Lipshitz we have:
$$
-s^k / 4 \le a(x^k) - a(x^{k} + \alpha_{P}^{k} d_{x}^{k})  \le s^k / 4
$$
which shows that $s^{k+1} \in [s^k/2, 2 s^k]$. Observe that $y^{k+1} = y^{k} + \alpha^k d_{y}^k \ge y^k / 2$. It remains to show that $\| y^{k+1} s^{k+1} - \mu^{k+1} \|_{\infty} \le \mu^k / 2$. Now we have:
$$
d_{y}  = - Y (S^{-1} d_{s} + e)
$$
Hence using that $\| d_{s} \| \le ...$ we get $d_{y} \in [-2y, 2y]$. It follows that $y^k + \alpha_{P} d_{y} \in [ y^{k} / 2, 3 y^{k} / 2]$. 

Finally, using the fact that $s^{k+1}  \in s^{k} [3/4, 5/4]$ and $s^{k+1} \in y^{k} [3/4, 5/4]$  we have: 
$$
\frac{s^{k+1} y^{k+1}}{s^k y^k} \in [1/2, 3/2]
$$
And since $\frac{s^{k} y^{k}}{\mu^k} \in [1/2,3/2]$ we have  $\frac{s^{k+1} y^{k+1}}{\mu^k} \in [1/4,3]$ which concludes the proof.
\end{proof}



\subsection{Trust region}

\begin{algorithm}[H]
\begin{algorithmic}
\Function{Stable-trust-region-step}{$\obj, \cons, x, y, r$}
\For{$j = 1, ..., \infty$}
\State $(d_{x}, d_{y}, M^{+}) \gets  \Call{Approx-Primal-Dual-Trust-Region}{\obj, \cons, x, y, r}$
\State $x^{+}, y^{+}, \alpha^{+} \gets \text{stable-line-search}(...)$
\If{$\alpha^{+} > \alpha_{\min}$}
\If{$j = 1$ \& $\alpha = 1$}
\State $r^{+} \gets 10 r$
\ElsIf{$\alpha^{+} < \alpha_{small}$}
\State $r^{+} \gets r / 2$
\Else
\State $r^{+} \gets \| d_{x} \|_{2}$
\EndIf
\State \algorithmicbreak
\EndIf
\State $r^{+} \gets r^{+} / 8$
\EndFor
\State \Return{$(x^{+}, y^{+}, M^{+},  r^{+})$}
\EndFunction
\end{algorithmic}
\caption{Stable-trust-region-step}
\end{algorithm}

\begin{algorithm}[H]
\begin{algorithmic}
\Function{Stable-trust-region-ipopt-style}{$\obj, \cons, x, y, r$}
\For{$j = 1, ..., \infty$}
\State Factorize $M$ with ... $\delta$ ...
\If{inertia is good}
\State ...
\EndIf
\State $x^{+}, y^{+}, \alpha^{+} \gets \text{stable-line-search}(...)$
\EndFor
\State \Return{$(x^{+}, y^{+}, M^{+},  r^{+})$}
\EndFunction
\end{algorithmic}
\caption{Stable-trust-region-step}
\end{algorithm}

\subsection{Line searches}

\begin{algorithm}[H]
\begin{algorithmic}
\Function{Move}{$\obj, \cons, x, y, d_{x}, d_{y}, \eta, \alpha_{P}$}
\State $x^{+} \gets x + \alpha_{P} d_{x}$
\State $\mu^{+} \gets (1 - \eta \alpha_{P}) \mu$
\State $\theta^{+} \gets (1 - \eta \alpha_{P}) \theta$
\State $s^{+} \gets a(x^{+}) + \theta (  s^{1} - a(x^{1}) )$
\State $\alpha_{D} \gets \arg \max_{\alpha \in [0,1]} {\alpha} \text{ s.t. } \frac{S^{+} (y + d_{y} \alpha_{D})}{\mu} \in [ e \parComp, e / \parComp ] $
\State $y^{+} \gets y + \alpha_{D} d_{y}$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\begin{algorithmic}
\Function{Dual-line-search}{$\obj, \cons, x^{+}, s^{+}, y, d_{y}, \alpha_{P}$}
\State $u_{D}, \status1 \gets \arg \max_{\alpha \in [0,1]} {\alpha} \text{ s.t. } \frac{S^{+} (y + d_{y} \alpha_{D})}{\mu} \in [ e \parComp, e / \parComp ] $
\State $l_{D}, \status2 \gets \arg \min_{\alpha \in [0,1]} {\alpha} \text{ s.t. } \frac{S^{+} (y + d_{y} \alpha_{D})}{\mu} \in [ e \parComp, e / \parComp ] $
\State $\alpha_{D} \gets \arg \min_{ \alpha \in [l_{D}, u_{D}] }{ D_{\gamma}(x^{+}, y  + \alpha d_{y} ) }$
\State $y^{+} \gets y + \alpha_{D} d_{y}$
\EndFunction
\end{algorithmic}
\end{algorithm}





\begin{algorithm}[H]
\begin{algorithmic}
\Function{Stable-line-search}{$\obj, \cons, x, y, s, d_{x}, d_{y}, d_{s} \eta$}
\State $\eta \gets 0$
\State $\alpha_{P} \gets \Call{FractionToBoundary}{y, s, d_{y}, d_{s} }$
\For{$i = 1, ..., \infty$}
\State  $x^{+}, y^{+}, \status \gets \Call{Move}{\obj, \cons, x, y, d_{x}, d_{y}, \eta, \alpha_{P}}$
\If{$\status = \feasible$}
\If{\text{sufficient progress on merit function}}
\State \Return{$x^{+}, y^{+}$}
\Else
\State 
\EndIf
\Else
\State $\alpha_{P} \gets \alpha_{P} /2$
\EndIf
\State ...
\EndFor
\EndFunction
\end{algorithmic}
\caption{Stable line search}
\end{algorithm}



\section{Scrap paper}

\section{Log barrier sub-problems}

This paper is concerned with the following problem:
\begin{subequations}\label{solve-problem}
\begin{flalign}
& \min{f(x) - \mu \log( s )} + \frac{1}{2} d_{x}^T D_{x} d_{x} + \frac{1}{2} d_{s}^T D_{s} d_{s} \\
& a(x)  - s = r \mu \\
& s \ge 0
\end{flalign}
\end{subequations}
The KKT conditions for \eqref{solve-problem} are:
\begin{subequations}\label{KKT-barrier}
\begin{flalign}
\nabla_{x} \Lag(x,y) = \nabla f(x) + D_{x} d_{x}  - \nabla a(x)^T y &= 0 \\
\ResComp_{\mu}(s, y) = Y s - \mu e &= 0  \\
\ResPrimal_{\mu}(x, s) = a(x) - s - \mu r &= 0 \\ 
s, y &\ge 0
\end{flalign}
\end{subequations}
Where the Lagrangian $\Lag(x,y) := f(x) - y^T a(x)$.

We combine the log barrier merit function and the complementary conditions as follows:
\begin{flalign}
\phi(x,y) = \psi(x) + \MeritComp(x,y)
\end{flalign}
With:
$$
\MeritComp(x,y) = \frac{\| \ResComp(x, y) \|_{\infty}^3}{\mu^2}
$$

We now introduces models to locally approximate these merit functions $\nabla_{x} \Lag(x,y)$, $\psi$, $\ResComp$ and $\phi$ respectively. To describe our approximations of a function $f$ around the point $(x, y)$ we use the function $\tilde{\Delta}_{(x,y)}^{f}(u, v)$ to denote the predicted increase in the function $f$ at the new point $(x + u, y + v)$. Observe that we use different approximations depending on the choice of function $f$.


We use a typical linear approximate of $\nabla_{x} \Lag(x,y)$ as follows:
\begin{flalign}
\tilde{\Delta}_{(x,y)}^{\nabla_{x} \Lag} (d_{x}, d_{y}) = \nabla_{x,x} L(x,y) d_{x} + \nabla a(x) d_{y}
\end{flalign}
The following function $\tilde{\Delta}_{(x,y)}^{\psi} ( u )$ is an approximation of the function $\psi(x)$ at the point $(x,y)$ and predicts how much the function $\psi$ changes as we change the current from $x$ to $x + u$.
\begin{flalign}
\tilde{\Delta}_{(x,y)}^{\psi} ( u ) = \frac{1}{2} u^T M(x, y) u + \nabla \psi(x)^T u
\end{flalign}

With:
\begin{flalign}
M (x,y) = \nabla^2 \Lag (x, y) + \sum_i{ \frac{y_i}{a(x)} \nabla a(x)^T \nabla a(x) }
\end{flalign}  
Note that if we set $y_i = \frac{\mu}{s_i}$ then $M(x,y) = \nabla^2 \psi(x)$ and $\tilde{\Delta}_{(x,y)}^{\psi}$ becomes the second order taylor approximation of $\psi$ at the point $x$. Thus we can think of $\tilde{\Delta}_{(x,y)}^{\psi} ( u )$ as a primal-dual approximation of the function $\psi$. 

We can also build a model of the $\MeritComp(x,y) $ as follows:
\begin{flalign}
\tilde{\Delta}^{\MeritComp}_{(x,y)}( d_{x}, d_{y} ) = \frac{\| S y + Y d_{s} + S d_{y} - \mu e \|_{\infty}^3 - \| \ResComp(x, y)  \|_{\infty}^3}{\mu^2}
\end{flalign}
With $S$ a diagonal matrix containing entries of $a(x)$ and $d_{s} = \nabla a(x) d_{x}$. This model $\tilde{\Delta}^{\ResComp}_{(x,y)}$ corresponds to the typical primal-dual linear model of $\ResComp$ i.e. $C(x + d_{x}, y + d_{y}) \approx S y + Y d_{s} + S d_{y} - \mu e$.

With $S$ and $Y$ contain the diagonal elements of $a(x)$ and $y$ respectively.

This allows us to approximate the change in the function $\phi$ at the point $(x,y)$ as follows:
\begin{flalign}
\tilde{\Delta}^{\phi}_{(x,y)}(d_{x}, d_{y}) = \tilde{\Delta}^{\psi}_{(x,y)}( d_{x} ) +   \tilde{\Delta}^{\MeritComp}_{(x,y)}( d_{x}, d_{y} )
\end{flalign}

We say an iterate $(x, y)$ satisfies approximate complementary if $(x,y) \in \Q$ where $\Q$ is defined as follows:
\begin{flalign}\label{approximate-complementary}
\Q = \left\{ (x, y) \in \R^{\NumVar} \times \R^{\NumCon} : a(x) > 0, y > 0, \| \ResComp(x, y) \|_{\infty} \le \frac{\mu}{2} \right\}
 \end{flalign}
We say the point $(x, y)$ is a $\mu$-scaled KKT point if $(x,y) \in \T$ where: 
\begin{flalign}\label{first-order-necessary}
\T = \left\{ (x, y) \in \Q :  \| \nabla \Lag (x, y) \| \le \mu ( \| y \|_1 + 1)  \right\}
 \end{flalign}
 
In which case the algorithm terminates.
 
\section{Algorithm}\label{sec:alg}

Let $S$, $Y$ denote the diagonal matrices with entries of $s$ and $y$ respectively. We can linearize \eqref{KKT-barrier} at the iterate $(x, y, s)$ as follows:
\begin{flalign}
\begin{bmatrix}
\nabla^2 \Lag (\hat{x}, \hat{y}) + D_{x} &  -\nabla a(\hat{x})^T & 0 \\
\nabla a(\hat{x}) & 0 & -I \\
0 & \hat{S} & \hat{Y} + D_{s}
\end{bmatrix} 
\begin{bmatrix}
d_x \\
d_y \\
d_s
\end{bmatrix}
&= -\begin{bmatrix}
\nabla \Lag(x, y) \\
\ResPrimal_{\mu}(x, s)  \\
\ResComp_{\mu}(s, y)
\end{bmatrix}
\end{flalign}

Which is equivalent to solving:

\begin{flalign}
\begin{bmatrix}
\nabla^2 \Lag (\hat{x}, \hat{y}) + \nabla a(x)^T D_{s} \nabla a(x) + D_{x}  &  \nabla a(\hat{x})^T  \\
\nabla a(\hat{x}) & -(\hat{Y}  + D_{s})^{-1}  \hat{S}  \\
\end{bmatrix} 
\begin{bmatrix}
d_x \\
-d_y 
\end{bmatrix}
&= -\begin{bmatrix}
\nabla \Lag(x, y) \\
\ResPrimal_{\mu}(x, s)  + (\hat{Y} + D_{s})^{-1} \ResComp_{\mu}(s, y)
\end{bmatrix}
\end{flalign}

One can also solve this system by solving the Schur complement:
$$
(\nabla^2 \Lag (\hat{x}, \hat{y}) + \nabla a(\hat{x})^T (\hat{Y}  + D_{s}) \hat{S}^{-1} \nabla a(\hat{x})  + D_{x} ) d_{x}  = -\nabla \Lag(x, \mu S^{-1} e) - \nabla a(\hat{x})^T  \hat{Y} \hat{S}^{-1} \ResPrimal_{\mu}(x, s) 
$$



Observe that \eqref{newton-system} may be singular or correspond to a direction that makes the log barrier objective worse. To rectify this problem we compute the direction as follows:
\begin{subequations}\label{compute-directions}
\begin{flalign}
& d_x = \arg \min_{\| u \|_{2} \le r}{ \tilde{\Delta}^{\psi}_{(x,y)} (u) } \label{compute-directions-dx} \\
& d_s = \nabla a(x) d_{x} \\
& d_y =  -S^{-1} \left( Y d_{s} + \ResComp (x, y) \right) \label{compute-dy}
\end{flalign}
\end{subequations}

******CAREFUL WITH SIGNS i.e. should be $d_{s} = - \nabla a(x) d_{x}$,  $d_{y} = -S^{-1} \left( Y d_{s} + \ResComp (x, y) \right)$ ***************

It is well-known from trust region literature that there exists some $\delta \in [0, \infty)$ such that:
\begin{flalign}\label{trust-region-linear-system}
(M(x,y) + \delta I) d_x = -\nabla \psi(x)
\end{flalign}
Furthermore, by re-arranging this equation we can deduce that $(d_x, d_y, d_s)$ satisfies a perturbed version of \eqref{newton-system}:
\begin{flalign}\label{perturbed-newton-system}
\begin{bmatrix}
\nabla^2 \Lag (x, y) + \delta I & -\nabla a(x)^T & 0  \\
-\nabla a(x) & 0 & I \\
0 & S & Y
\end{bmatrix} 
\begin{bmatrix}
d_x \\
d_y \\
d_s
\end{bmatrix}
&=  -\begin{bmatrix}
\nabla \Lag(x, y) \\
0 \\
\ResComp(x, y)
\end{bmatrix}
\end{flalign}

\begin{algorithm}[H]
\caption{Primal-dual trust region step}\label{AlgTrust}
\begin{algorithmic}
\Function{\AlgTrust}{$x, y, r$}
\begin{subequations}\label{compute-directions}
**** $\in$ ****
\begin{flalign}
& d_x \in \arg \min_{\| u \| \le r}{ \tilde{\Delta}^{\psi}_{(x, y)} ( u ) } \label{compute-directions-dx} \\
& d_s = \nabla a(x) d_{x} \\
& S = \Diag(a(x)) \\
& d_y =  - S^{-1} \left( Y d_{s} + \ResComp (x, y) \right)
\end{flalign}
\end{subequations}
\State $(x^{+}, y^{+}) \gets (x + d_{x}, y + d_{y})$
\State $\Return (x^{+}, y^{+}, d_{x}, d_{y})$
\EndFunction
\end{algorithmic}
\end{algorithm}

Our complete algorithm is summarized as follows:

\begin{algorithm}[H]
\begin{algorithmic}\label{AlgMain}
\Function{\AlgMain}{$x^1, y^1$}
\For{$k = 1, ... , \infty$}
\State $r \gets R (y^k)$
\Repeat
\State $(x^{+}, y^{+}, d_{x}, d_{y}) \gets \callAlgTrust{x^k, y^k, r}$
\If{$(x^{+}, y^{+}) \in \Q$}
\If{$(x^{+}, y^{+}) \in \T$}
\State \Return{$(x^{+}, y^{+})$}
\EndIf
\EndIf
\State $r \gets r / 2$
\Until{$\phi(x^{+}) > \phi(x^k) + \frac{1}{2} \tilde{\Delta}_{(x^k,y^k)}^{\phi}  ( d_{x}, d_{y} )$}
\State $x^{k} \gets x^{+}$
\State $y^k \gets y^{+}$
\EndFor
\EndFunction
\end{algorithmic}
\caption{Primal-dual non-convex interior point algorithm}
\end{algorithm}


\section{Delta computation}

\begin{algorithm}[H]
\begin{algorithmic}
\State $\gamma_{lb} = 0$, $\gamma_{ub} = \delta_{\max} = \| H \|^2_{F}$, $\delta_{k-1}$ \Comment{lower and upper bounds on minimum eigenvalue}
\State Try $\delta = 0$, if succeeds, trial solve with this delta. If step size is small skip to trust region step.
\State $\delta = \delta_{k-1}$
 \If{$\delta = 0$}
\State $\delta = \delta_{\min}$
\EndIf
\For{$i = 1, ..., \infty$}
\State Break if inertia correct and update $\gamma_{lb}$ and $\gamma_{ub}$.
\State $\delta = \delta 100$
\EndFor
\State \text{Trust region}
\State $R = \| d_{x}^{k-1} \|_{2}$
\For{$i = 1, ..., \infty$}
\State Compute trust region with $R$
\State If trust region is too accurate increase radius size
\State If step unsuccessful decrease radius size
\State Prevent oscillation
\EndFor
\end{algorithmic}
\caption{Delta}
\end{algorithm}

\end{document}
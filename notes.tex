\documentclass{article}

\input{preamble.tex}

\begin{document}

\title{A one phase IPM for non-convex optimization}
\author{Oliver Hinder, Yinyu Ye}

\maketitle

\newcommand{\algorithmicbreak}{\textbf{break}}
\newcommand{\obj}{f}
\newcommand{\cons}{a}
\newcommand{\parNumCor}{p(\#corrections)}
\newcommand{\parComp}{\beta_{1}}
\newcommand{\parCompAgg}{\beta_{2}}
\newcommand{\status}{\textbf{status}}
\newcommand{\feasible}{\textbf{feasible}}

\abstract{
Solver
}

\section{Main algorithm}
$$
D_{\lambda}(x,y) =  \frac{\| \nabla L(x,y) \|_{\infty}}{\sqrt{ \| y \|_{\infty} + 1 } }
$$
$$
E_{\mu}(x,y,s) = \max\left\{ D(x,y), \| S y - \mu \|_{\infty}  \right\}
$$

\begin{algorithm}[H]
\begin{algorithmic}\label{AlgMain}
\Function{IPM}{$x, y$}
\State $\lambda \gets ...$
\State $f_{\lambda}(x) := f(x) + \lambda \| x \|^2$
\For{$k = 1, ... \infty$}
\State Form primal schur complement and factorize.
\If{ matrix is PD }
\State \Call{Stable-correction}{}
\Else
\State \Call{Stable-trust-region-step}{}
\EndIf
\For{$i = 1, ..., \parNumCor$}
\If{$D_{\lambda}(x,y) \le \min\{ \mu, \theta \}$ \& $\frac{Sy}{\mu} \in [ \parCompAgg , 1/ \parCompAgg ]$ }
\State \Call{Aggressive-correction}{}
\State $\lambda \gets ...$
\Else
\State \Call{Stable-correction}{}
%\State Compute stable direction by solving [system] with $\eta = 0$.
%\State $(x, y) \gets \Call{Stable-line-search}{}$
\EndIf
\EndFor
\EndFor
\EndFunction
\end{algorithmic}
\caption{One phase primal-dual IPM}
\end{algorithm}

\subsection{Trust region}

\begin{algorithm}[H]
\begin{algorithmic}
\Function{Stable-trust-region-step}{$\obj, \cons, x, y, r$}
\For{$j = 1, ..., \infty$}
\State $(d_{x}, d_{y}, M^{+}) \gets  \Call{Approx-Primal-Dual-Trust-Region}{\obj, \cons, x, y, r}$
\State $x^{+}, y^{+}, \alpha^{+} \gets \text{stable-line-search}(...)$
\If{$\alpha^{+} > \alpha_{\min}$}
\If{$j = 1$ \& $\alpha = 1$}
\State $r^{+} \gets 10 r$
\ElsIf{$\alpha^{+} < \alpha_{small}$}
\State $r^{+} \gets r / 2$
\Else
\State $r^{+} \gets \| d_{x} \|_{2}$
\EndIf
\State \algorithmicbreak
\EndIf
\State $r^{+} \gets r^{+} / 8$
\EndFor
\State \Return{$(x^{+}, y^{+}, M^{+},  r^{+})$}
\EndFunction
\end{algorithmic}
\caption{Stable-trust-region-step}
\end{algorithm}

\begin{algorithm}[H]
\begin{algorithmic}
\Function{Stable-trust-region-ipopt-style}{$\obj, \cons, x, y, r$}
\For{$j = 1, ..., \infty$}
\State Factorize $M$ with ... $\delta$ ...
\If{inertia is good}
\State ...
\EndIf
\State $x^{+}, y^{+}, \alpha^{+} \gets \text{stable-line-search}(...)$
\EndFor
\State \Return{$(x^{+}, y^{+}, M^{+},  r^{+})$}
\EndFunction
\end{algorithmic}
\caption{Stable-trust-region-step}
\end{algorithm}

\subsection{Line searches}

\begin{algorithm}[H]
\begin{algorithmic}
\Function{Move}{$\obj, \cons, x, y, d_{x}, d_{y}, \eta, \alpha_{P}$}
\State $x^{+} \gets x + \alpha_{P} d_{x}$
\State $\mu^{+} \gets (1 - \eta \alpha_{P}) \mu$
\State $\theta^{+} \gets (1 - \eta \alpha_{P}) \theta$
\State $s^{+} \gets a(x^{+}) + \theta (  s^{1} - a(x^{1}) )$
\State $\alpha_{D} \gets \arg \max_{\alpha \in [0,1]} {\alpha} \text{ s.t. } \frac{S^{+} (y + d_{y} \alpha_{D})}{\mu} \in [ e \parComp, e / \parComp ] $
\State $y^{+} \gets y + \alpha_{D} d_{y}$
\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\begin{algorithmic}
\Function{Aggressive-line-search}{$\obj, \cons, x, y, d_{x}, d_{y}$}
\State $\eta \gets 1$
\State $\alpha_{P} \gets \Call{FractionToBoundary}{s, d_{s} }$
\For{$i = 1, ..., \infty$}
\State $x^{+}, y^{+}, \status \gets \Call{Move}{\obj, \cons, x, y, d_{x}, d_{y}, \eta, \alpha_{P}}$
\If{$\status = \feasible$ \& Function value does not increase too much}
\State \Return{$x^{+}, y^{+}, s^{+}$}
\Else
\State $\alpha_{P} \gets \alpha_{P} / 2$
\EndIf
\EndFor
\EndFunction
\end{algorithmic}
\caption{Aggressive line search}
\end{algorithm}


\begin{algorithm}[H]
\begin{algorithmic}
\Function{Stable-line-search}{$\obj, \cons, x, y, s, d_{x}, d_{y}, d_{s} \eta$}
\State $\eta \gets 0$
\State $\alpha_{P} \gets \Call{FractionToBoundary}{y, s, d_{y}, d_{s} }$
\For{$i = 1, ..., \infty$}
\State  $x^{+}, y^{+}, \status \gets \Call{Move}{\obj, \cons, x, y, d_{x}, d_{y}, \eta, \alpha_{P}}$
\If{$\status = \feasible$}
\If{\text{sufficient progress on merit function}}
\State \Return{$x^{+}, y^{+}$}
\Else
\State 
\EndIf
\Else
\State $\alpha_{P} \gets \alpha_{P} /2$
\EndIf
\State ...
\EndFor
\EndFunction
\end{algorithmic}
\caption{Stable line search}
\end{algorithm}



\section{Scrap paper}

\section{Log barrier sub-problems}

This paper is concerned with the following problem:
\begin{subequations}\label{solve-problem}
\begin{flalign}
& \min{f(x) - \mu \log( s )} + \frac{1}{2} d_{x}^T D_{x} d_{x} + \frac{1}{2} d_{s}^T D_{s} d_{s} \\
& a(x)  - s = r \mu \\
& s \ge 0
\end{flalign}
\end{subequations}
The KKT conditions for \eqref{solve-problem} are:
\begin{subequations}\label{KKT-barrier}
\begin{flalign}
\nabla_{x} \Lag(x,y) = \nabla f(x) + D_{x} d_{x}  - \nabla a(x)^T y &= 0 \\
\ResComp_{\mu}(s, y) = Y s - \mu e &= 0  \\
\ResPrimal_{\mu}(x, s) = a(x) - s - \mu r &= 0 \\ 
s, y &\ge 0
\end{flalign}
\end{subequations}
Where the Lagrangian $\Lag(x,y) := f(x) - y^T a(x)$.

We combine the log barrier merit function and the complementary conditions as follows:
\begin{flalign}
\phi(x,y) = \psi(x) + \MeritComp(x,y)
\end{flalign}
With:
$$
\MeritComp(x,y) = \frac{\| \ResComp(x, y) \|_{\infty}^3}{\mu^2}
$$

We now introduces models to locally approximate these merit functions $\nabla_{x} \Lag(x,y)$, $\psi$, $\ResComp$ and $\phi$ respectively. To describe our approximations of a function $f$ around the point $(x, y)$ we use the function $\tilde{\Delta}_{(x,y)}^{f}(u, v)$ to denote the predicted increase in the function $f$ at the new point $(x + u, y + v)$. Observe that we use different approximations depending on the choice of function $f$.


We use a typical linear approximate of $\nabla_{x} \Lag(x,y)$ as follows:
\begin{flalign}
\tilde{\Delta}_{(x,y)}^{\nabla_{x} \Lag} (d_{x}, d_{y}) = \nabla_{x,x} L(x,y) d_{x} + \nabla a(x) d_{y}
\end{flalign}
The following function $\tilde{\Delta}_{(x,y)}^{\psi} ( u )$ is an approximation of the function $\psi(x)$ at the point $(x,y)$ and predicts how much the function $\psi$ changes as we change the current from $x$ to $x + u$.
\begin{flalign}
\tilde{\Delta}_{(x,y)}^{\psi} ( u ) = \frac{1}{2} u^T M(x, y) u + \nabla \psi(x)^T u
\end{flalign}

With:
\begin{flalign}
M (x,y) = \nabla^2 \Lag (x, y) + \sum_i{ \frac{y_i}{a(x)} \nabla a(x)^T \nabla a(x) }
\end{flalign}  
Note that if we set $y_i = \frac{\mu}{s_i}$ then $M(x,y) = \nabla^2 \psi(x)$ and $\tilde{\Delta}_{(x,y)}^{\psi}$ becomes the second order taylor approximation of $\psi$ at the point $x$. Thus we can think of $\tilde{\Delta}_{(x,y)}^{\psi} ( u )$ as a primal-dual approximation of the function $\psi$. 

We can also build a model of the $\MeritComp(x,y) $ as follows:
\begin{flalign}
\tilde{\Delta}^{\MeritComp}_{(x,y)}( d_{x}, d_{y} ) = \frac{\| S y + Y d_{s} + S d_{y} - \mu e \|_{\infty}^3 - \| \ResComp(x, y)  \|_{\infty}^3}{\mu^2}
\end{flalign}
With $S$ a diagonal matrix containing entries of $a(x)$ and $d_{s} = \nabla a(x) d_{x}$. This model $\tilde{\Delta}^{\ResComp}_{(x,y)}$ corresponds to the typical primal-dual linear model of $\ResComp$ i.e. $C(x + d_{x}, y + d_{y}) \approx S y + Y d_{s} + S d_{y} - \mu e$.

With $S$ and $Y$ contain the diagonal elements of $a(x)$ and $y$ respectively.

This allows us to approximate the change in the function $\phi$ at the point $(x,y)$ as follows:
\begin{flalign}
\tilde{\Delta}^{\phi}_{(x,y)}(d_{x}, d_{y}) = \tilde{\Delta}^{\psi}_{(x,y)}( d_{x} ) +   \tilde{\Delta}^{\MeritComp}_{(x,y)}( d_{x}, d_{y} )
\end{flalign}

We say an iterate $(x, y)$ satisfies approximate complementary if $(x,y) \in \Q$ where $\Q$ is defined as follows:
\begin{flalign}\label{approximate-complementary}
\Q = \left\{ (x, y) \in \R^{\NumVar} \times \R^{\NumCon} : a(x) > 0, y > 0, \| \ResComp(x, y) \|_{\infty} \le \frac{\mu}{2} \right\}
 \end{flalign}
We say the point $(x, y)$ is a $\mu$-scaled KKT point if $(x,y) \in \T$ where: 
\begin{flalign}\label{first-order-necessary}
\T = \left\{ (x, y) \in \Q :  \| \nabla \Lag (x, y) \| \le \mu ( \| y \|_1 + 1)  \right\}
 \end{flalign}
 
In which case the algorithm terminates.
 
\section{Algorithm}\label{sec:alg}

Let $S$, $Y$ denote the diagonal matrices with entries of $s$ and $y$ respectively. We can linearize \eqref{KKT-barrier} at the iterate $(x, y, s)$ as follows:
\begin{flalign}
\begin{bmatrix}
\nabla^2 \Lag (\hat{x}, \hat{y}) + D_{x} &  -\nabla a(\hat{x})^T & 0 \\
\nabla a(\hat{x}) & 0 & -I \\
0 & \hat{S} & \hat{Y} + D_{s}
\end{bmatrix} 
\begin{bmatrix}
d_x \\
d_y \\
d_s
\end{bmatrix}
&= -\begin{bmatrix}
\nabla \Lag(x, y) \\
\ResPrimal_{\mu}(x, s)  \\
\ResComp_{\mu}(s, y)
\end{bmatrix}
\end{flalign}

Which is equivalent to solving:

\begin{flalign}
\begin{bmatrix}
\nabla^2 \Lag (\hat{x}, \hat{y}) + \nabla a(x)^T D_{s} \nabla a(x) + D_{x}  &  \nabla a(\hat{x})^T  \\
\nabla a(\hat{x}) & -(\hat{Y}  + D_{s})^{-1}  \hat{S}  \\
\end{bmatrix} 
\begin{bmatrix}
d_x \\
-d_y 
\end{bmatrix}
&= -\begin{bmatrix}
\nabla \Lag(x, y) \\
\ResPrimal_{\mu}(x, s)  + (\hat{Y} + D_{s})^{-1} \ResComp_{\mu}(s, y)
\end{bmatrix}
\end{flalign}

One can also solve this system by solving the Schur complement:
$$
(\nabla^2 \Lag (\hat{x}, \hat{y}) + \nabla a(\hat{x})^T (\hat{Y}  + D_{s}) \hat{S}^{-1} \nabla a(\hat{x})  + D_{x} ) d_{x}  = -\nabla \Lag(x, \mu S^{-1} e) - \nabla a(\hat{x})^T  \hat{Y} \hat{S}^{-1} \ResPrimal_{\mu}(x, s) 
$$



Observe that \eqref{newton-system} may be singular or correspond to a direction that makes the log barrier objective worse. To rectify this problem we compute the direction as follows:
\begin{subequations}\label{compute-directions}
\begin{flalign}
& d_x = \arg \min_{\| u \|_{2} \le r}{ \tilde{\Delta}^{\psi}_{(x,y)} (u) } \label{compute-directions-dx} \\
& d_s = \nabla a(x) d_{x} \\
& d_y =  -S^{-1} \left( Y d_{s} + \ResComp (x, y) \right) \label{compute-dy}
\end{flalign}
\end{subequations}

******CAREFUL WITH SIGNS i.e. should be $d_{s} = - \nabla a(x) d_{x}$,  $d_{y} = -S^{-1} \left( Y d_{s} + \ResComp (x, y) \right)$ ***************

It is well-known from trust region literature that there exists some $\delta \in [0, \infty)$ such that:
\begin{flalign}\label{trust-region-linear-system}
(M(x,y) + \delta I) d_x = -\nabla \psi(x)
\end{flalign}
Furthermore, by re-arranging this equation we can deduce that $(d_x, d_y, d_s)$ satisfies a perturbed version of \eqref{newton-system}:
\begin{flalign}\label{perturbed-newton-system}
\begin{bmatrix}
\nabla^2 \Lag (x, y) + \delta I & -\nabla a(x)^T & 0  \\
-\nabla a(x) & 0 & I \\
0 & S & Y
\end{bmatrix} 
\begin{bmatrix}
d_x \\
d_y \\
d_s
\end{bmatrix}
&=  -\begin{bmatrix}
\nabla \Lag(x, y) \\
0 \\
\ResComp(x, y)
\end{bmatrix}
\end{flalign}

\begin{algorithm}[H]
\caption{Primal-dual trust region step}\label{AlgTrust}
\begin{algorithmic}
\Function{\AlgTrust}{$x, y, r$}
\begin{subequations}\label{compute-directions}
**** $\in$ ****
\begin{flalign}
& d_x \in \arg \min_{\| u \| \le r}{ \tilde{\Delta}^{\psi}_{(x, y)} ( u ) } \label{compute-directions-dx} \\
& d_s = \nabla a(x) d_{x} \\
& S = \Diag(a(x)) \\
& d_y =  - S^{-1} \left( Y d_{s} + \ResComp (x, y) \right)
\end{flalign}
\end{subequations}
\State $(x^{+}, y^{+}) \gets (x + d_{x}, y + d_{y})$
\State $\Return (x^{+}, y^{+}, d_{x}, d_{y})$
\EndFunction
\end{algorithmic}
\end{algorithm}

Our complete algorithm is summarized as follows:

\begin{algorithm}[H]
\begin{algorithmic}\label{AlgMain}
\Function{\AlgMain}{$x^1, y^1$}
\For{$k = 1, ... , \infty$}
\State $r \gets R (y^k)$
\Repeat
\State $(x^{+}, y^{+}, d_{x}, d_{y}) \gets \callAlgTrust{x^k, y^k, r}$
\If{$(x^{+}, y^{+}) \in \Q$}
\If{$(x^{+}, y^{+}) \in \T$}
\State \Return{$(x^{+}, y^{+})$}
\EndIf
\EndIf
\State $r \gets r / 2$
\Until{$\phi(x^{+}) > \phi(x^k) + \frac{1}{2} \tilde{\Delta}_{(x^k,y^k)}^{\phi}  ( d_{x}, d_{y} )$}
\State $x^{k} \gets x^{+}$
\State $y^k \gets y^{+}$
\EndFor
\EndFunction
\end{algorithmic}
\caption{Primal-dual non-convex interior point algorithm}
\end{algorithm}


\section{Delta computation}

\begin{algorithm}[H]
\begin{algorithmic}
\State $\lambda_{lb} = 0$, $\lambda_{ub} = \delta_{\max} = \| H \|^2_{F}$, $\delta_{k-1}$ \Comment{lower and upper bounds on minimum eigenvalue}
\State Try $\delta = 0$, if succeeds, trial solve with this delta. If step size is small skip to trust region step.
\State $\delta = \delta_{k-1}$
 \If{$\delta = 0$}
\State $\delta = \delta_{\min}$
\EndIf
\For{$i = 1, ..., \infty$}
\State Break if inertia correct and update $\lambda_{lb}$ and $\lambda_{ub}$.
\State $\delta = \delta 100$
\EndFor
\State \text{Trust region}
\State $R = \| d_{x}^{k-1} \|_{2}$
\For{$i = 1, ..., \infty$}
\State Compute trust region with $R$
\State If trust region is too accurate increase radius size
\State If step unsuccessful decrease radius size
\State Prevent oscillation
\EndFor
\end{algorithmic}
\caption{Delta}
\end{algorithm}

\end{document}